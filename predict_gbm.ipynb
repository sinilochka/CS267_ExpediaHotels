{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification of Expedia hotels: application of Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "to predict 3 classes being\n",
    "\n",
    "0 - no action\n",
    "\n",
    "1 - click\n",
    "\n",
    "2 - booking\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define multiclass labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_class(row):\n",
    "    click = row['click_bool']\n",
    "    book = row['booking_bool']\n",
    "    if int(book) == 1:\n",
    "        return 2\n",
    "    elif int(book) == 0 and int(click) == 1:\n",
    "        return 1\n",
    "    elif int(book) == 0 and int(click) == 0:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Extract balanced sample from train.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Click + booking samples\n",
    "part_1 = data[data['click_bool'] == 1]\n",
    "\n",
    "# No action samples\n",
    "select_rows = int(200000)\n",
    "part_2 = data[data['click_bool'] == 0].iloc[:select_rows, :]\n",
    "\n",
    "result = pd.concat([part_1, part_2])\n",
    "result['class'] = result.apply(multi_class, axis=1)\n",
    "result.to_csv('data/sample_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643672, 55)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <th>booking_bool</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>68914</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>114.29</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-12-31 08:59:22</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>139893</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6</td>\n",
       "      <td>2013-06-05 12:27:51</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>104251</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>162.38</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>8</td>\n",
       "      <td>2013-03-20 17:50:44</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>27669</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>96.41</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>11</td>\n",
       "      <td>2013-02-25 08:39:33</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>20499</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "12        1  2013-04-04 08:32:15       12                          187   \n",
       "59        4  2012-12-31 08:59:22        5                          219   \n",
       "63        6  2013-06-05 12:27:51       14                          100   \n",
       "68        8  2013-03-20 17:50:44        5                          219   \n",
       "90       11  2013-02-25 08:39:33        5                          219   \n",
       "\n",
       "    visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
       "12                      NaN                   NaN              219    68914   \n",
       "59                      NaN                   NaN              219   139893   \n",
       "63                      NaN                   NaN              100   104251   \n",
       "68                      NaN                   NaN              219    27669   \n",
       "90                      NaN                   NaN              219    20499   \n",
       "\n",
       "    prop_starrating  prop_review_score  ...    comp7_rate  comp7_inv  \\\n",
       "12                2                3.0  ...           NaN        NaN   \n",
       "59                2                3.0  ...           NaN        NaN   \n",
       "63                3                4.0  ...           NaN        NaN   \n",
       "68                3                3.5  ...           NaN        NaN   \n",
       "90                2                3.5  ...           NaN        NaN   \n",
       "\n",
       "    comp7_rate_percent_diff  comp8_rate  comp8_inv  comp8_rate_percent_diff  \\\n",
       "12                      NaN         0.0        0.0                     11.0   \n",
       "59                      NaN         NaN        NaN                      NaN   \n",
       "63                      NaN         NaN        NaN                      NaN   \n",
       "68                      NaN         0.0        0.0                      NaN   \n",
       "90                      NaN         0.0        0.0                      NaN   \n",
       "\n",
       "    click_bool  gross_bookings_usd  booking_bool  class  \n",
       "12           1              114.29             1      2  \n",
       "59           1                 NaN             0      1  \n",
       "63           1              162.38             1      2  \n",
       "68           1               96.41             1      2  \n",
       "90           1                 NaN             0      1  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load balanced data: target and features. Drop unnecessary variables to form a valid feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/sample_1.csv')\n",
    "target = np.ravel(train['class'].values)\n",
    "train = train.drop(['date_time', 'click_bool', 'gross_bookings_usd', 'booking_bool', 'class'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing values with a negative value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = train.fillna(-100).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test with test being 30% of the train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, \n",
    "                                                                     target, test_size = 0.3, random_state = 3456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((193102, 50), (193102,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
       "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=3421, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(random_state=3421)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Finding best loss and optimizing # of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " N-estimators\tLogLoss Score on Train\tLogLoss Score on Test\tAccuracy Score Train\tAccuracy Score Test\n",
      "30 \t0.336983368152 \t0.339102023468 \t0.891559580087 \t0.890741680563\n",
      "60 \t0.316209709266 \t0.318845499504 \t0.891617284773 \t0.890788288055\n",
      "90 \t0.312489707631 \t0.315623761489 \t0.891723816499 \t0.890907396091\n",
      "120 \t0.310883365518 \t0.314551549051 \t0.891856981157 \t0.890954003584\n",
      "150 \t0.309965571941 \t0.314149819156 \t0.891945757596 \t0.890959182194\n",
      "180 \t0.309210668759 \t0.313903822382 \t0.892054508733 \t0.891026504127\n",
      "210 \t0.308542716215 \t0.313706592239 \t0.89213884635 \t0.891104183281\n",
      "240 \t0.30787271815 \t0.313522727441 \t0.892216525734 \t0.891145612164\n",
      "270 \t0.307333253548 \t0.313396145631 \t0.892311960406 \t0.891145612164\n",
      "300 \t0.306815746578 \t0.313318782964 \t0.892374103913 \t0.891171505215\n"
     ]
    }
   ],
   "source": [
    "print \"N-estimators\\tLogLoss Score on Train\\tLogLoss Score on Test\\tAccuracy Score Train\\tAccuracy Score Test\"\n",
    "for i in xrange(30,301,30):\n",
    "    clf = GradientBoostingClassifier(n_estimators=i, random_state=3421)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_predictions = clf.predict_proba(X_train)\n",
    "    test_predictions = clf.predict_proba(X_test)\n",
    "    train_pred = clf.predict(X_train)\n",
    "    test_pred = clf.predict(X_test)\n",
    "    print i,\"\\t\", log_loss(y_train, train_predictions), \"\\t\", log_loss(y_test, test_predictions), \"\\t\", accuracy_score(y_train, train_pred), \"\\t\", accuracy_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate = 0.05\n",
      "Min Samples Split\tLogLoss Score Train\tLogLoss Score Test\tAccuracy Score Train\tAccuracy Score Test\n",
      "2 \t0.319348850761 \t0.321817284157 \t0.891568457731 \t0.890752037783\n",
      "3 \t0.319348850761 \t0.321817284157 \t0.891568457731 \t0.890752037783\n",
      "4 \t0.319348850761 \t0.321817284157 \t0.891568457731 \t0.890752037783\n",
      "Learning Rate = 0.01\n",
      "Min Samples Split\tLogLoss Score Train\tLogLoss Score Test\tAccuracy Score Train\tAccuracy Score Test\n",
      "2 \t0.478601996728 \t0.479818044743 \t0.891559580087 \t0.890741680563\n",
      "3 \t0.478601996728 \t0.479818044743 \t0.891559580087 \t0.890741680563\n",
      "4 \t0.478601996728 \t0.479818044743 \t0.891559580087 \t0.890741680563\n"
     ]
    }
   ],
   "source": [
    "for learning_rate in [0.05, 0.01]:\n",
    "    print \"Learning Rate =\", learning_rate\n",
    "    print \"Min Samples Split\\tLogLoss Score Train\\tLogLoss Score Test\\tAccuracy Score Train\\tAccuracy Score Test\"\n",
    "    for i in xrange(2,5):\n",
    "        clf = GradientBoostingClassifier(min_samples_split=i, learning_rate=learning_rate, n_estimators=100, random_state=3421)\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_predictions = clf.predict_proba(X_train)\n",
    "        test_predictions = clf.predict_proba(X_test)\n",
    "        train_pred = clf.predict(X_train)\n",
    "        test_pred = clf.predict(X_test)\n",
    "        print i,\"\\t\", log_loss(y_train, train_predictions), \"\\t\", log_loss(y_test, test_predictions), \"\\t\", accuracy_score(y_train, train_pred), \"\\t\", accuracy_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
